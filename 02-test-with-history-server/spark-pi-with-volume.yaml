apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi-with-pvc
  namespace: default
spec:
  type: Scala
  mode: cluster
  ## Docker official image 
  ## (https://hub.docker.com/layers/library/spark/3.5.3/images/sha256-980d682bd4e1a89537c13cf1e555a32bef7b329f21a21dc99b9192c93e62c26e)
  image: apache/spark:latest
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.4.jar
  ## The SparkApplication object will be garbage collected 
  ## if the current time is more than the .spec.timeToLiveSeconds since its termination.
  timeToLiveSeconds: 3600
  ## eventLog is enabled
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:/tmp"
  ## hostPath is used for testing purposes. All pods are in the same node.
  volumes:
    - name: example-volume
      hostPath:
        path: /data # directory location on host
        type: Directory # this field is optional
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: 512m
    memoryOverhead: 128m  
    serviceAccount: spark
    ## Mountpath for driver
    ## Where will data be stored:
    volumeMounts:
      - name: example-volume
        mountPath: /tmp
    # securityContext:
    #   allowPrivilegeEscalation: false
    #   runAsUser: 0
  executor:
    cores: 1
    memory: 512m
    instances: 1
    memoryOverhead: 128m
    serviceAccount: spark
    ## Mountpath for executor
    volumeMounts:
      - name: example-volume
        mountPath: /tmp
    # securityContext:
    #   allowPrivilegeEscalation: false
    #   runAsUser: 0