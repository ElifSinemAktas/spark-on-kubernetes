apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi-with-pvc
  namespace: default
spec:
  type: Scala
  mode: cluster
  ## Docker official image 
  ## (https://hub.docker.com/layers/library/spark/3.5.3/images/sha256-980d682bd4e1a89537c13cf1e555a32bef7b329f21a21dc99b9192c93e62c26e)
  image: spark:3.5.3
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples.jar
  sparkVersion: "3.5.3"
  ## The SparkApplication object will be garbage collected 
  ## if the current time is more than the .spec.timeToLiveSeconds since its termination.
  timeToLiveSeconds: 3600
  ## eventLog is enabled
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:/opt/spark/logs"
    "spark.jars.ivy": "/tmp/.ivy"
  ## volume points to pvc we created
  volumes:
    - name: spark-logs
      persistentVolumeClaim:
        claimName: spark-logs-pvc
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: 512m
    memoryOverhead: 128m  
    serviceAccount: spark
    ## Mountpath for driver
    volumeMounts:
      - name: spark-logs
        mountPath: /opt/spark/logs
    ## securityContext (temporarily for test, "permission denied")
    podSecurityContext:
      fsGroup: 1000
      runAsUser: 1000
  executor:
    cores: 1
    memory: 512m
    instances: 1
    memoryOverhead: 128m
    serviceAccount: spark
    ## Mountpath for executor
    volumeMounts:
      - name: spark-logs
        mountPath: /opt/spark/logs
    podSecurityContext:
      fsGroup: 1000
      runAsUser: 1000
    

